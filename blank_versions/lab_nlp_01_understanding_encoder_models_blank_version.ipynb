{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benoitmialet/Statistical-and-data-analysis-using-R-/blob/main/blank_versions/lab_nlp_01_understanding_encoder_models_blank_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa367622",
      "metadata": {
        "id": "aa367622"
      },
      "source": [
        "# LAB NLP 01 Understanding encoder models\n",
        "# -Version with blanks-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925d8e25",
      "metadata": {
        "id": "925d8e25"
      },
      "source": [
        "## Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7bcffd",
      "metadata": {
        "id": "db7bcffd"
      },
      "source": [
        "In this lab session we are going to use our first transformer model on a locally!\n",
        "\n",
        "Understanding model structure and functioning is required before trying to use it for inference or training. This is the objective of this lab. In the next one, we will work with some Natural Language Processing tasks, where transformer models have shown outstading results since several years. In these Labs, we will first focus on Encoder-only models. Then, we will tackle Encoder-Decoder models and Decoder-only models.\n",
        "\n",
        "All along the Labs, we will massively use Hugging Face Hub and Hugging Face libraries to try out different transformer models.\n",
        "* **Hugging Face Hub** is a huge storage for Open Source models: https://huggingface.co/models. Anyone can upload a model, for public or private usage. Filters can be activated to expore models: tasks, languages, licenses, etc. Task page provide a nice overview: https://huggingface.co/tasks\n",
        "* **Hugging Face librairies** offer a universal implementation that allow to use any model from the Hub, providing your hardware  can handle it: https://huggingface.co/docs. By using a quite simple syntax, you will be able to perform any NLP task (or Speech to text, computer vision, etc.). We will essentially use `Transformers` and `Datasets` libraries in this purpose.\n",
        "\n",
        "Note: at any moment, if you face an \"out of memory\" error, just reset the notebook kernel. It is also advised to reset the notebook kernel each time you load a new model, to avoid memory crash."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e8146b",
      "metadata": {
        "id": "19e8146b"
      },
      "source": [
        "## Discovering BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a68ac602",
      "metadata": {
        "id": "a68ac602"
      },
      "source": [
        "To start with NLP encoder models, BERT is the most known example (Google, 2018). As a reminder, **BERT encoder has been trained for masked language modeling (MLM)** task in English language, which made him good at feature extraction, that's to say good at extracting the semantic meaning of a text and the information it contains (in English). It has also been trained for Next Sentence Prediction (SNP), but studies has shown that it was not as good as MLM.\n",
        "\n",
        "\n",
        "First thing to do before trying to use a transformer model is to understand how it works. Go to the BERT model page then read the text : https://huggingface.co/google-bert/bert-base-uncased/. See how this one is well documented! This is not the case for all models.\n",
        "\n",
        "\n",
        "Next, you have to check the main model files on the \"Files and versions\" tab and understand their purpose. **To use NLP models, you will basically need these files:**\n",
        "\n",
        "* **config.json** shows model parameters configuration.\n",
        "\n",
        "* **tokenizer_config.json** indicates the max length of inputs you can feed the model with. It's one of the most important information. So, keep in mind \"**512** tokens\".\n",
        "\n",
        "* **tokenizer.json** is the most important file. It contains all the vocabulary and the mapping between token and their ids. It also contains all the special tokens. You had to read them carefully\n",
        "\n",
        "* **vocab.txt** references all the tokens of the vocabulary.\n",
        "\n",
        "* At least 1 model file. Most of the time, several model file types are available on the model repository. Each of them contains all the weights of the model. you don't need to download all of them. You have to pick at least one of them. Most common types are:\n",
        "  * **pytorch_model.bin** is the standard binary format (pickle). All repositories have this one.\n",
        "  * **model.safetensors** is a cool format created by Hugging Face. It is faster to load, especially on cpu. Another cool feature displays all the model layers just by clicking on the little icon on the hub, next to the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720c6077",
      "metadata": {
        "id": "720c6077"
      },
      "source": [
        "## Loading the model on device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79bc1fb",
      "metadata": {
        "id": "b79bc1fb"
      },
      "source": [
        "All methods that are used in the lab are in the next cell. If you lose one, reload the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a858f871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a858f871",
        "outputId": "517c4f32-2c22-4ae5-f81a-1844e50debcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoConfig, BertConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModel, AutoModelForMaskedLM, BertForMaskedLM,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# this line of code will be useful in any of your projects, to check GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456644a9",
      "metadata": {
        "id": "456644a9"
      },
      "source": [
        "`AutoModel` class can load any model if a model name or path is provided. It automatically identifies model type and loads it, thanks to `from_pretrained` method. This method has plenty of optional parameters than can be found on the documentation, or with `help(AutoModel.from_pretrained)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8977f4cb",
      "metadata": {
        "id": "8977f4cb"
      },
      "outputs": [],
      "source": [
        "# help(AutoModel.from_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2846f5c",
      "metadata": {
        "id": "b2846f5c"
      },
      "source": [
        "Most of the time, this high-level class works perfectly.\n",
        "\n",
        "* `use_safetensors` allows to load model with safetensor files. You can also load tensorflow version, etc.\n",
        "* the `to()` method moves the data in a specific device (cpu, gpu). Be careful: **model and input data MUST be on the same device**. tokenizer stays on cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google-bert/bert-base-uncased\""
      ],
      "metadata": {
        "id": "Vt_Lka3akY0y"
      },
      "id": "Vt_Lka3akY0y",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd521e0f",
      "metadata": {
        "id": "dd521e0f"
      },
      "outputs": [],
      "source": [
        "# Load model with the `from_pretrained` method\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f9eb16",
      "metadata": {
        "id": "59f9eb16"
      },
      "source": [
        "WARNING: During the model loading, if a warning message states that some layer were randomly initialized, unfortunately the model is not correctly automatically recognized. You will need to use a specific import class, unless you do it on purpose, for instance to initialize a new layer before training it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a016cd",
      "metadata": {
        "id": "87a016cd"
      },
      "source": [
        "Looking at BERT architecture, we observe:\n",
        "* The **embedding** part, that outputs token embedding.\n",
        "* The **\"body\"**, that update token embeddings (feature extraction), with 12 blocks (\"Layers\"), each containing several attention heads\n",
        "* The **\"head\"**, That is trained for a specific task. Here, it is trained to pool all the token embeddings of the input sequence into 1 single vector, with only 1 pooler layer.\n",
        "\n",
        "Note: model body can be seen as a drill, ad the head as a drill bit. For a same body, an infinite variety of heads can be trained. **Body does the feature extraction and head does a specific task**. Task can be pooling, pooling + classification, or anything else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4abf11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e4abf11",
        "outputId": "1ae4f16e-3f1d-42a9-e444-db6123c05023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "bert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd62f16",
      "metadata": {
        "id": "1bd62f16"
      },
      "source": [
        "More details can be found, like the number of attention heads per decoder block, by loading model config file. They are different ways to do it:\n",
        "* by checking `config.json` file.\n",
        "* using `model.config` attribute\n",
        "* by instanciating `AutoConfig` object. `AutoConfig` is a class that contains `config.json` information. It can automatically identify the model type just like `AutoModel` and `AutoTokenizer`. However, `BertConfig` will work only for BERT. Both will give the same result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13102e00",
      "metadata": {
        "id": "13102e00"
      },
      "outputs": [],
      "source": [
        "# Load Bert Autoconfig object.\n",
        "\n",
        "# bert_config =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d34dda",
      "metadata": {
        "id": "c3d34dda"
      },
      "source": [
        "## Using a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9ffa3ff",
      "metadata": {
        "id": "d9ffa3ff"
      },
      "source": [
        "Tokenization is the first step of processing input data (text), before converting it into a numeric format.\n",
        "\n",
        "Each model family (e.g. BERT) has its own tokenizer. You will have to identify which tokenizer or how tokenizer works for each model.\n",
        "\n",
        "3 main tokenization techniques exist:\n",
        "* Character tokenization: splitting text into characters\n",
        "* Word tokenization: splitting text into words\n",
        "* **Subword tokenization**: splitting text into word or smaller entities. It combines best aspects of word and character tokenization and is now broadly adopted.\n",
        "\n",
        "N.B.: Word and Subword tokenizers themselves are trained along the model training process, with the same text corpus.\n",
        "N.B.: Today, Subword tokenization is the main technique used in most NLP models.\n",
        "\n",
        "--"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703085a0",
      "metadata": {
        "id": "703085a0"
      },
      "source": [
        "Tokenizer is an object itself and must be imported apart from the model, to prepare model input data. There are two options to load files from the Hub:\n",
        "1. Download locally the model directory using `git clone`, `wget`... and provide the directory path to the import method.\n",
        "2. Provide the `model_id` to the same import method. `model_id` is simply the model name displayed on the hub (\"google-bert/bert-base-uncased\"). The model will be downloaded in a cache then loaded. That's what we are going to do now."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12977b14",
      "metadata": {
        "id": "12977b14"
      },
      "source": [
        "`AutoTokenizer` automatically detects the type of tokenizer a model use, just by provinding it's path or model_id.\n",
        "if we us `print()` on the tokenizer object, we will access to all the important details. Let's have a look:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b3e17e8",
      "metadata": {
        "id": "0b3e17e8"
      },
      "source": [
        "### WordPiece Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b88209",
      "metadata": {
        "id": "48b88209"
      },
      "source": [
        "WordPiece Tokenizer is generally used by models focusing on 1 language, mostly English, but it can be another european language too. Examples: BERT, DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d4d6749",
      "metadata": {
        "id": "1d4d6749"
      },
      "outputs": [],
      "source": [
        "# load bert tokenizer\n",
        "\n",
        "# bert_tokenizer ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6570b131",
      "metadata": {
        "id": "6570b131"
      },
      "source": [
        "Some characteristics appear, like the vocabulary size. But plenty of information can be found using `help()`. That's where the WordPiece type of tokenizer appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "463dc3a3",
      "metadata": {
        "id": "463dc3a3"
      },
      "outputs": [],
      "source": [
        "# help(bert_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d0e164",
      "metadata": {
        "id": "f3d0e164"
      },
      "source": [
        "#### Special tokens\n",
        "\n",
        "Special tokens are particular tokens, not always related to words. They have to be known to understand tokenization. Special tokens differs from a model to another. Here we take a WordPiece tokenizer as example. Here are the special tokens:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae0725e",
      "metadata": {
        "id": "1ae0725e"
      },
      "source": [
        "* `[CLS]` is the Classification token. It is put at the beginning of input sequence, and will be used if the model task is classification (the rest of the sequence is discarded). It integrates the global context of the sequence\n",
        "* `[SEP]` is the separator token. It delimitates where sentences end up in a sequence. It also marks the end of the input sequence.\n",
        "* `[UKN]` is the unknown token. It is used to handle words or subwords that are not in the model's vocabulary\n",
        "* `[PAD]` is the padding token. When providing a batch of multiple samples as input in the model, all the tensors must have the same size. A max_size is set (by default or manually), and all sequences shortest than this will be completed by this token.\n",
        "* `[MASK]` is the mask token. It is randomly put in a sequence if the model is being trained for Masked Language Modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee438e28",
      "metadata": {
        "id": "ee438e28"
      },
      "source": [
        "Let's tokenize a sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dec7d57f",
      "metadata": {
        "id": "dec7d57f"
      },
      "outputs": [],
      "source": [
        "# tokenize the following sentence:\n",
        "text = \"tokenizing text is a core task in NLP.\"\n",
        "\n",
        "# tokens =\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c819ac4",
      "metadata": {
        "id": "4c819ac4"
      },
      "source": [
        "We can see `[CLS]`, `[SEP]` and some tokens beginning by `##`, which indicates they are subword tokens, which are linked to the previous token.\n",
        "\n",
        "We can also observe that each token has its own mapped id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4f06c444",
      "metadata": {
        "id": "4f06c444"
      },
      "outputs": [],
      "source": [
        "# display token_id for each encoded token, using a list comprehension, or a pandas DataFrame\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e212c6ef",
      "metadata": {
        "id": "e212c6ef"
      },
      "source": [
        "### SentencePiece tokenizer (XLM-RoBERTa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e3302e",
      "metadata": {
        "id": "f6e3302e"
      },
      "source": [
        "SentencePiece Tokenizer is generally (not exclusively) used by models that can handle multiple languages, including non european languages. Examples: mBERT, XLM-RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b332c5a2",
      "metadata": {
        "id": "b332c5a2",
        "outputId": "fd0c6e80-893f-4242-c58c-eec118610cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaTokenizerFast(name_or_path='FacebookAI/xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model_id = \"FacebookAI/xlm-roberta-base\"\n",
        "xlmroberta_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "xlmroberta_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0xH0s5102v0J"
      },
      "id": "0xH0s5102v0J"
    },
    {
      "cell_type": "markdown",
      "id": "c9e4d0dc",
      "metadata": {
        "id": "c9e4d0dc"
      },
      "source": [
        "Here, we see different special tokens, but thanks to the `special_tokens` dictionnary, we can understand that classification. For instance, BOS and SEP tokens are now `<s>` and `</s>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "20a29b28",
      "metadata": {
        "id": "20a29b28"
      },
      "outputs": [],
      "source": [
        "# tokenize then print tokens for the following sentence:\n",
        "\n",
        "text = \"tokenizing text is a core task in NLP.\"\n",
        "\n",
        "# tokens =\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399f8442",
      "metadata": {
        "id": "399f8442"
      },
      "source": [
        "Now it's different from WordPiece. Tokens corresponding to the beginning of a word starts with `_`, and those who are linked to the previous one doesn't. The SentencePiece tokenizer is agnostic to accents, ponctuation, and is more adappted to languages without whitespaces, like Japanese."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14c3bc2",
      "metadata": {
        "id": "b14c3bc2"
      },
      "source": [
        "## Understanding BERT model input and output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09407fc8",
      "metadata": {
        "id": "09407fc8"
      },
      "source": [
        "Let's use the famous BERT model and try to understand the output data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf70d3e",
      "metadata": {
        "id": "6cf70d3e"
      },
      "source": [
        "### Tokenization (input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec22feb",
      "metadata": {
        "id": "eec22feb"
      },
      "source": [
        "Tokenized data can be returned in different formats. We will use pytorch one.\n",
        "\n",
        "Don't forget to move tokenized data, which will be model input data, on the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0f3fd686",
      "metadata": {
        "id": "0f3fd686",
        "outputId": "9ae5d834-4234-424f-a3f2-86fd7338b0c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  4553, 17953,  2361,  1012,   102,  2009,  1005,  1055,\n",
              "          1037,  3255, 10126,  1012,  2009,  2003,  2061,  2524,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "text1 = \"I learn NLP.\"\n",
        "text2 = \"It's a pain everyday. It is so hard!\"\n",
        "tokens = bert_tokenizer(text1, text2, return_tensors='pt').to(\"cpu\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36619604",
      "metadata": {
        "id": "36619604"
      },
      "source": [
        "`token_type_ids` `attention_mask` and are used for training. We will not use them in the lab. For your information:\n",
        "* `token_type_ids` identifies sequences if several has been tokenized at once.\n",
        "* `attention_mask` will indicate which token are masked (for MLM training)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c014f56e",
      "metadata": {
        "id": "c014f56e"
      },
      "source": [
        "We see some special tokens (`101`, `102`) among them. Let's decode them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c1ec0bd2",
      "metadata": {
        "id": "c1ec0bd2"
      },
      "outputs": [],
      "source": [
        "# decode `101` and `102` special tokens without explicitely use \"101\" and \"102\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1fa322f",
      "metadata": {
        "id": "a1fa322f"
      },
      "source": [
        "### Model output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a604298",
      "metadata": {
        "id": "4a604298"
      },
      "source": [
        "Before using a model for inference, puting code in a `torch.no_grad()` context will avoid computing gradients for nothing. However, it is not a big deal here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89df7f2",
      "metadata": {
        "id": "f89df7f2",
        "outputId": "478fc73b-3199-4950-e153-024325aebc83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.7 ms ± 3.34 ms per loop (mean ± std. dev. of 100 runs, 20 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 20 -r 100\n",
        "output = bert_model(tokens.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cce9c7e",
      "metadata": {
        "id": "8cce9c7e",
        "outputId": "fd22e047-e7c4-476e-92bd-653a2d6e1d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.7 ms ± 1.93 ms per loop (mean ± std. dev. of 100 runs, 20 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 20 -r 100\n",
        "with torch.no_grad():\n",
        "    output = bert_model(tokens.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfc8b80c",
      "metadata": {
        "id": "bfc8b80c"
      },
      "source": [
        "Reminder: encoder model are build in two parts:\n",
        "* A **\"body\"** for feature extraction, with several blocks, each containing several attention heads\n",
        "* A **\"head\"** for classification (or anything else), pooling\n",
        "\n",
        "So, model output provides body and head outputs.\n",
        "* body output is given by `last_hidden_state` attribute\n",
        "* head output is given by `pooler_output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71acbd5",
      "metadata": {
        "id": "f71acbd5",
        "outputId": "126e90a1-ec47-468d-8497-e8e81595af77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "output = bert_model(tokens.input_ids)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2c04b4b9",
      "metadata": {
        "id": "2c04b4b9"
      },
      "outputs": [],
      "source": [
        "# display `last_hidden_state` dimension lengths and explain them\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121fe701",
      "metadata": {
        "id": "121fe701"
      },
      "source": [
        "`last_hidden_state` corresponds to the extracted text context.\n",
        "\n",
        "Dimensions are:\n",
        "* ?\n",
        "* ?\n",
        "* ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "cb9498db",
      "metadata": {
        "id": "cb9498db"
      },
      "outputs": [],
      "source": [
        "# display `pooler_output` dimension lengths and explain them\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5eb2ef",
      "metadata": {
        "id": "bc5eb2ef"
      },
      "source": [
        "`pooler_output` shape is fixed. This layer is trained to \"pool\" hidden state matrix into one single vector. This vector carries the text context and can be used for other purposes such as sentence similarity or text classification.\n",
        "\n",
        "Dimensions are:\n",
        "* ?\n",
        "* ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historically, classification token (`[CLS]` here) embedding vector was  the one to use for classification. `pooler_output` is a linear than Tahn transformation of this `[CLS]`vector, supposed to be less \"noisy\", and is preferred for classification."
      ],
      "metadata": {
        "id": "a0BDufwu9aJ2"
      },
      "id": "a0BDufwu9aJ2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ty to extract only the first embedding vector of each sequence of the batch, using tensor slicing []\n",
        "# print it's shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8EV5NKQ79YwU"
      },
      "id": "8EV5NKQ79YwU",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "74acf4bc",
      "metadata": {
        "id": "74acf4bc"
      },
      "source": [
        "Hugging Face Transformers can also provide all decoder blocks outputs and their attention matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b471c976",
      "metadata": {
        "id": "b471c976"
      },
      "outputs": [],
      "source": [
        "output = bert_model(\n",
        "    tokens.input_ids,\n",
        "    output_hidden_states=True,\n",
        "    output_attentions=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c2745d",
      "metadata": {
        "id": "26c2745d",
        "outputId": "f803f49e-8456-41f0-e02c-b5bffaddb532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 13\n"
          ]
        }
      ],
      "source": [
        "print(len(output.attentions), len(output.hidden_states))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}